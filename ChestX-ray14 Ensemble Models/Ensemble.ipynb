{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources:**\n",
    "\n",
    "The code is a further development of this preliminary project:\n",
    "\n",
    "[1] https://github.com/bjorneme/preliminary/tree/main\n",
    "\n",
    "The state-of-the-art model used as inspiration for multi-label classification on ChestX-ray14 is:\n",
    "\n",
    "[2] https://github.com/syednabilashraf/SynthEnsemble/tree/main\n",
    "\n",
    "The code used for TTA is:\n",
    "\n",
    "[3] https://github.com/taheeraahmed/master-thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import itertools\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc, f1_score\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "# Hugging Face transformers to load the MambaVision model\n",
    "from transformers import AutoModel, AutoConfig, AutoModelForImageClassification, set_seed\n",
    "\n",
    "# Import timm for models\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "ZIP_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14.zip'\n",
    "EXTRACTED_PATH = '/cluster/home/bjorneme/projects/Data/chestX-ray14-extracted'\n",
    "\n",
    "# Disease labels\n",
    "disease_labels = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "SEED = 42\n",
    "NUM_WORKERS = 32\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Seed for Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 14:34:06.567244: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 14:34:06.583588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747398846.602047  434504 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747398846.607721  434504 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-16 14:34:06.628043: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def seed_everything(seed=SEED):\n",
    "    \"\"\"\n",
    "    Sets the seed to ensure reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    set_seed(seed)\n",
    "\n",
    "# Apply the seed\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(zip_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Extracts the ZIP file of the dataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(extracted_path, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_path)\n",
    "\n",
    "# Extract ChestX-ray14 dataset\n",
    "# extract_data(ZIP_PATH, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(csv_path, extracted_path):\n",
    "    \"\"\"\n",
    "    Read labels from CSV, maps images to paths, and create binary disease labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the CSV containing labels\n",
    "    labels_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Create binary columns for each disease label\n",
    "    for disease in disease_labels:\n",
    "        labels_df[disease] = labels_df['Finding Labels'].str.contains(disease).astype(int)\n",
    "\n",
    "    # Create binary column for 'No Finding'\n",
    "    labels_df['No Finding'] = labels_df['Finding Labels'].str.contains('No Finding').astype(int)\n",
    "\n",
    "    # Map images to their full path\n",
    "    labels_df['Path'] = labels_df['Image Index'].map(\n",
    "        {os.path.basename(path): path for path in glob(os.path.join(extracted_path, '**', 'images', '*.png'))}\n",
    "    )\n",
    "    \n",
    "    return labels_df\n",
    "\n",
    "# Path to the labels CSV file\n",
    "labels_csv_path = os.path.join(EXTRACTED_PATH, 'Data_Entry_2017.csv')\n",
    "\n",
    "# Load and preprocess the labels\n",
    "df = load_labels(labels_csv_path, EXTRACTED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 78614. Number of unique patients: 21563\n",
      "Validation size: 11212. Number of unique patients: 3081\n",
      "Test size: 22294. Number of unique patients: 6161\n"
     ]
    }
   ],
   "source": [
    "# Split based on patients\n",
    "unique_patients = df['Patient ID'].unique()\n",
    "\n",
    "# Split patients into training, validation and test sets\n",
    "train_val_patients, test_patients = train_test_split(\n",
    "    unique_patients, test_size=0.2, random_state=SEED\n",
    ")\n",
    "train_patients, val_patients = train_test_split(\n",
    "    train_val_patients, test_size=0.125, random_state=SEED\n",
    ")\n",
    "\n",
    "# Create dataframes for training, validation, and test sets\n",
    "train_df = df[df['Patient ID'].isin(train_patients)].reset_index(drop=True)\n",
    "val_df = df[df['Patient ID'].isin(val_patients)].reset_index(drop=True)\n",
    "test_df = df[df['Patient ID'].isin(test_patients)].reset_index(drop=True)\n",
    "\n",
    "# Verify Split Sizes\n",
    "print(f\"Train dataset size: {len(train_df)}. Number of unique patients: {len(train_patients)}\")\n",
    "print(f\"Validation size: {len(val_df)}. Number of unique patients: {len(val_patients)}\")\n",
    "print(f\"Test size: {len(test_df)}. Number of unique patients: {len(test_patients)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Dataset for Chest X-ray images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Create dataset for Chest X-ray images.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Get image and labels\n",
    "        img_path = self.df.iloc[idx]['Path']\n",
    "        image = plt.imread(img_path)\n",
    "        label = self.df.iloc[idx][disease_labels].values.astype(np.float32)\n",
    "        \n",
    "        # Apply transformation on image\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Test Transformations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "# Define transformations for test data\n",
    "test_transforms = transforms.Compose([\n",
    "\n",
    "    # Convert image to PIL format\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Convert to 3 channels\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "\n",
    "    # Resize the image to 256x256\n",
    "    transforms.Resize((256,256)),\n",
    "\n",
    "    # Create 10 crops\n",
    "    transforms.TenCrop(224),\n",
    "    transforms.Lambda(lambda crops: torch.stack([\n",
    "        transforms.ToTensor()(crop) for crop in crops\n",
    "    ])),\n",
    "\n",
    "    # Normalize using ImageNet mean and std\n",
    "    transforms.Lambda(lambda crops: torch.stack(\n",
    "        [transforms.Normalize(mean, std)(crop) for crop in crops]\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ChestXrayDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Test DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Build the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all models\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load DenseNet121**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using DenseNet121 as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained DenseNet121 model\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1).to((device)).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.classifier= nn.Linear(self.base_model.classifier.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "densenet121_model = MultiLabelClassifier(device)\n",
    "densenet121_model = nn.DataParallel(densenet121_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "densenet121_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/DenseNet121/densenet121_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['DenseNet121'] = densenet121_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load MambaVision L**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/cluster/home/bjorneme/.conda/envs/master_thesis/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using MambaVision as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name=\"nvidia/MambaVision-T2-1K\", num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained MambaVision model\n",
    "        self.base_model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.model.head = nn.Linear(self.base_model.model.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_pool, _ = self.base_model(x)\n",
    "        return self.base_model.model.head(avg_pool)\n",
    "\n",
    "# Initialize the Model\n",
    "mambavision_L_model = MultiLabelClassifier(device, \"nvidia/MambaVision-L-21K\")\n",
    "mambavision_L_model = nn.DataParallel(mambavision_L_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "mambavision_L_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/MambaVision_Large/mambavision_L_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['MambaVision L'] = mambavision_L_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load CoAtNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using CoAtNet as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained CoAtNet model\n",
    "        self.base_model = timm.create_model(model_name, pretrained=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.head.fc = nn.Linear(self.base_model.head.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "coatnet_model = MultiLabelClassifier(device, \"coatnet_2_rw_224.sw_in12k_ft_in1k\")\n",
    "coatnet_model = nn.DataParallel(coatnet_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "coatnet_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/CoAtNet/coatnet_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['CoAtNet'] = coatnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import ConvNeXt v2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using ConvNeXt v2 as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained ConvNeXt v2 model\n",
    "        self.base_model = timm.create_model(model_name, pretrained=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.head.fc = nn.Linear(self.base_model.head.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "coatnet_model = MultiLabelClassifier(device, \"convnextv2_large.fcmae_ft_in22k_in1k\")\n",
    "coatnet_model = nn.DataParallel(coatnet_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "coatnet_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/ConvNeXt_v2/convnext_v2_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['ConvNeXt v2'] = coatnet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load MaxViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using MaxViT as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained MaxViT model\n",
    "        self.base_model = timm.create_model(model_name, pretrained=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.head.fc = nn.Linear(self.base_model.head.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "maxvit_model = MultiLabelClassifier(device, \"maxxvitv2_rmlp_base_rw_224.sw_in12k_ft_in1k\")\n",
    "maxvit_model = nn.DataParallel(maxvit_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "maxvit_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/MaxViT/maxvit_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['MaxViT'] = maxvit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Swin v2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using Swin v2 as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained Swin v2 model\n",
    "        self.base_model = timm.create_model(model_name, pretrained=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.head.fc = nn.Linear(self.base_model.head.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Initialize the Model\n",
    "swin_v2_model = MultiLabelClassifier(device, \"swinv2_cr_small_ns_224.sw_in1k\")\n",
    "swin_v2_model = nn.DataParallel(swin_v2_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "swin_v2_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/Swin_v2/swin_v2_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['Swin v2'] = swin_v2_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load VMamba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/bjorneme/.cache/huggingface/modules/transformers_modules/saurabhati/VMamba_ImageNet_83.6/7d7eb526013d331199d56ce43baaf8463bfc324f/modeling_vmamba.py:694: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd\n",
      "/cluster/home/bjorneme/.cache/huggingface/modules/transformers_modules/saurabhati/VMamba_ImageNet_83.6/7d7eb526013d331199d56ce43baaf8463bfc324f/modeling_vmamba.py:709: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_bwd\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Label Classification Model using Vision Mamba as the base model.\n",
    "    \"\"\"\n",
    "    def __init__(self, device, model_name, num_classes=len(disease_labels)):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        # Load pre-trained Vision Mamba model\n",
    "        self.base_model = AutoModelForImageClassification.from_pretrained(model_name,trust_remote_code=True).to(device)\n",
    "\n",
    "        # Replace the classification head to match the number of disease labels\n",
    "        self.base_model.head = nn.Linear(self.base_model.head.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x).logits\n",
    "\n",
    "# Initialize the Model\n",
    "vmamba_model = MultiLabelClassifier(device, \"saurabhati/VMamba_ImageNet_83.6\")\n",
    "vmamba_model = nn.DataParallel(vmamba_model).to(device)\n",
    "\n",
    "# Load the model\n",
    "vmamba_model.load_state_dict(torch.load(\n",
    "    '../ChestX-ray14 Single Models/VMamba/vmamba_tta_backbone.pt',\n",
    "    weights_only=True\n",
    "))\n",
    "\n",
    "all_models['VMamba'] = vmamba_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the test set.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store all predictions\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(test_loader, desc=f\"Evaluating {model_name} on Test Set\")\n",
    "\n",
    "    # Set to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in progress_bar:\n",
    "            # Retrieve input sizes\n",
    "            batch_size, ncrops, C, H, W = inputs.size()\n",
    "\n",
    "            # Move to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Change to [batch_size * ncrops, C, H, W]\n",
    "            inputs = inputs.view(-1, C, H, W)  \n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Change to [batch_size * ncrops, C, H, W] and average\n",
    "            outputs = outputs.view(batch_size, ncrops, -1).mean(1)\n",
    "\n",
    "            # Apply sigmoid\n",
    "            predictions = torch.sigmoid(outputs)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            all_preds.append(predictions.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    predictions = torch.cat(all_preds)\n",
    "    labels = torch.cat(all_labels)\n",
    "    \n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate all Models on the Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DenseNet121 on Test Set:   0%|          | 0/697 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating DenseNet121 on Test Set: 100%|██████████| 697/697 [02:48<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MambaVision L on Test Set: 100%|██████████| 697/697 [14:04<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating CoAtNet on Test Set: 100%|██████████| 697/697 [05:57<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating ConvNeXt v2 on Test Set: 100%|██████████| 697/697 [20:20<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating MaxViT on Test Set: 100%|██████████| 697/697 [11:51<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Swin v2 on Test Set: 100%|██████████| 697/697 [07:17<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating VMamba on Test Set: 100%|██████████| 697/697 [09:12<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_all_models(all_models, test_loader):\n",
    "    \"\"\"\n",
    "    Do the evaluation loop on all models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dictionaries for storing predictions. Do also store labels\n",
    "    predictions_dict = {}\n",
    "    labels_dict = {}\n",
    "\n",
    "    # Evaluate all models\n",
    "    for model_name, model in all_models.items():\n",
    "\n",
    "        # Retrive predictions and true labels\n",
    "        predictions, labels = evaluate_model(model, test_loader, model_name)\n",
    "\n",
    "        # Save predictions and labels\n",
    "        predictions_dict[model_name] = predictions\n",
    "        labels_dict[model_name] = labels\n",
    "\n",
    "        print()\n",
    "\n",
    "    return predictions_dict, labels_dict\n",
    "\n",
    "# Evaluate all models\n",
    "predictions_dict, labels_dict = evaluate_all_models(all_models, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the Best Combination of Ensembles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:   0%|          | 0/127 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:   2%|▏         | 2/127 [00:01<00:57,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121',), mAUC: 0.8405\n",
      "New best combination: ('MambaVision L',), mAUC: 0.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:   3%|▎         | 4/127 [00:01<00:30,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('CoAtNet',), mAUC: 0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:   7%|▋         | 9/127 [00:02<00:19,  6.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121', 'MambaVision L'), mAUC: 0.8533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  12%|█▏        | 15/127 [00:02<00:16,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('MambaVision L', 'CoAtNet'), mAUC: 0.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  18%|█▊        | 23/127 [00:04<00:14,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('CoAtNet', 'VMamba'), mAUC: 0.8563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  24%|██▎       | 30/127 [00:05<00:14,  6.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121', 'MambaVision L', 'CoAtNet'), mAUC: 0.8569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  35%|███▌      | 45/127 [00:07<00:12,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('MambaVision L', 'CoAtNet', 'ConvNeXt v2'), mAUC: 0.8571\n",
      "New best combination: ('MambaVision L', 'CoAtNet', 'MaxViT'), mAUC: 0.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  38%|███▊      | 48/127 [00:07<00:11,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('MambaVision L', 'CoAtNet', 'VMamba'), mAUC: 0.8584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  52%|█████▏    | 66/127 [00:10<00:08,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121', 'MambaVision L', 'CoAtNet', 'MaxViT'), mAUC: 0.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  54%|█████▎    | 68/127 [00:10<00:08,  7.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121', 'MambaVision L', 'CoAtNet', 'VMamba'), mAUC: 0.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  69%|██████▊   | 87/127 [00:13<00:06,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('MambaVision L', 'CoAtNet', 'ConvNeXt v2', 'VMamba'), mAUC: 0.8585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  70%|███████   | 89/127 [00:13<00:05,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('MambaVision L', 'CoAtNet', 'MaxViT', 'VMamba'), mAUC: 0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination:  82%|████████▏ | 104/127 [00:16<00:03,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best combination: ('DenseNet121', 'MambaVision L', 'CoAtNet', 'MaxViT', 'VMamba'), mAUC: 0.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Searching for the best combination: 100%|██████████| 127/127 [00:19<00:00,  6.54it/s]\n"
     ]
    }
   ],
   "source": [
    "def find_best_model(predictions_dict, labels_dict):\n",
    "    \"\"\"\n",
    "    Search for the best combination of models.\n",
    "    \"\"\"\n",
    "\n",
    "    best_auc = 0\n",
    "    metric_df_best_combination = None\n",
    "    best_model_combination = None\n",
    "\n",
    "    # Model names\n",
    "    model_names = list(predictions_dict.keys())\n",
    "\n",
    "    # Find model combinations\n",
    "    all_combinations = []\n",
    "    for i in range(1, len(model_names)+1):\n",
    "        for combination in itertools.combinations(model_names, i):\n",
    "            all_combinations.append(combination)\n",
    "\n",
    "    # Progress bar\n",
    "    progress_bar = tqdm(all_combinations, desc=f\"Searching for the best combination\")\n",
    "    \n",
    "    # Iterate over all model combinations\n",
    "    for combination in progress_bar:\n",
    "\n",
    "        # Dictionaries for storing metrics\n",
    "        accuracy_per_label, f1_score_per_label, mean_auc_per_label = {}, {}, {}\n",
    "\n",
    "        # Aggreage predictions for all models\n",
    "        ensemble_pred = 0\n",
    "        for model_name in combination:\n",
    "            ensemble_pred += predictions_dict[model_name]\n",
    "        predictions = ensemble_pred/ len(combination)\n",
    "\n",
    "        # Compute metrics for each label\n",
    "        for i, label in enumerate(disease_labels):\n",
    "\n",
    "            # Retrieve labels\n",
    "            labels = labels_dict[model_name]\n",
    "\n",
    "            # Create binary predictions\n",
    "            binary_predictions = predictions[:, i] > 0.5\n",
    "\n",
    "            # Calculate accuracy\n",
    "            accuracy_per_label[label] = accuracy_score(labels[:, i], binary_predictions)\n",
    "\n",
    "            # Calculate f1 score\n",
    "            f1_score_per_label[label] = f1_score(labels[:, i], binary_predictions, average='macro')\n",
    "\n",
    "            # Calculate mAUC\n",
    "            mean_auc_per_label[label] = roc_auc_score(labels[:, i], predictions[:, i])\n",
    "\n",
    "        # Create a df with metrics\n",
    "        metrics_df = pd.DataFrame({\n",
    "            'Accuracy': accuracy_per_label,\n",
    "            'F1 Score': f1_score_per_label,\n",
    "            'mAUC': mean_auc_per_label\n",
    "        })\n",
    "\n",
    "        # Calculate the mean for all diseases\n",
    "        metrics_df.loc['Overall'] = metrics_df.mean()\n",
    "\n",
    "        # Save if mAUC is higher\n",
    "        if metrics_df.loc['Overall']['mAUC'] > best_auc:\n",
    "            metric_df_best_combination = metrics_df\n",
    "            best_model_combination = combination\n",
    "            best_auc = metrics_df.loc['Overall']['mAUC']\n",
    "\n",
    "            # Print model combination and mAUC\n",
    "            print(f\"New best combination: {combination}, mAUC: {metrics_df.loc['Overall']['mAUC']:.4f}\")\n",
    "        \n",
    "    # Return the metric for the best model\n",
    "    return metric_df_best_combination, best_model_combination\n",
    "\n",
    "metric_df_best_combination, best_model_combination = find_best_model(predictions_dict, labels_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print Results Best Combination**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DenseNet121', 'MambaVision L', 'CoAtNet', 'MaxViT', 'VMamba')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>mAUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>0.905311</td>\n",
       "      <td>0.589978</td>\n",
       "      <td>0.842810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>0.957298</td>\n",
       "      <td>0.490139</td>\n",
       "      <td>0.831343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Infiltration</th>\n",
       "      <td>0.813851</td>\n",
       "      <td>0.542619</td>\n",
       "      <td>0.725643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>0.956087</td>\n",
       "      <td>0.614740</td>\n",
       "      <td>0.898554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edema</th>\n",
       "      <td>0.978828</td>\n",
       "      <td>0.509052</td>\n",
       "      <td>0.903311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emphysema</th>\n",
       "      <td>0.976316</td>\n",
       "      <td>0.643845</td>\n",
       "      <td>0.938433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fibrosis</th>\n",
       "      <td>0.983538</td>\n",
       "      <td>0.498560</td>\n",
       "      <td>0.832963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Effusion</th>\n",
       "      <td>0.902261</td>\n",
       "      <td>0.725109</td>\n",
       "      <td>0.891039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>0.986813</td>\n",
       "      <td>0.496681</td>\n",
       "      <td>0.771131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural_Thickening</th>\n",
       "      <td>0.970306</td>\n",
       "      <td>0.504261</td>\n",
       "      <td>0.822139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <td>0.971921</td>\n",
       "      <td>0.614828</td>\n",
       "      <td>0.927573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nodule</th>\n",
       "      <td>0.942944</td>\n",
       "      <td>0.571702</td>\n",
       "      <td>0.812478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass</th>\n",
       "      <td>0.953889</td>\n",
       "      <td>0.636508</td>\n",
       "      <td>0.868511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hernia</th>\n",
       "      <td>0.998026</td>\n",
       "      <td>0.620196</td>\n",
       "      <td>0.965331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Overall</th>\n",
       "      <td>0.949814</td>\n",
       "      <td>0.575587</td>\n",
       "      <td>0.859376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Accuracy  F1 Score      mAUC\n",
       "Atelectasis         0.905311  0.589978  0.842810\n",
       "Consolidation       0.957298  0.490139  0.831343\n",
       "Infiltration        0.813851  0.542619  0.725643\n",
       "Pneumothorax        0.956087  0.614740  0.898554\n",
       "Edema               0.978828  0.509052  0.903311\n",
       "Emphysema           0.976316  0.643845  0.938433\n",
       "Fibrosis            0.983538  0.498560  0.832963\n",
       "Effusion            0.902261  0.725109  0.891039\n",
       "Pneumonia           0.986813  0.496681  0.771131\n",
       "Pleural_Thickening  0.970306  0.504261  0.822139\n",
       "Cardiomegaly        0.971921  0.614828  0.927573\n",
       "Nodule              0.942944  0.571702  0.812478\n",
       "Mass                0.953889  0.636508  0.868511\n",
       "Hernia              0.998026  0.620196  0.965331\n",
       "Overall             0.949814  0.575587  0.859376"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(best_model_combination)\n",
    "metric_df_best_combination"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "master_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
